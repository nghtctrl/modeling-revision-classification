{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8022240,"sourceType":"datasetVersion","datasetId":4727324},{"sourceId":8256074,"sourceType":"datasetVersion","datasetId":4899555},{"sourceId":11384,"sourceType":"modelInstanceVersion","modelInstanceId":6216}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Modeling Revision Classification\n\nDaniel Kim, Jason G. Chew, Jiho Kim\n\n# Introduction","metadata":{}},{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom sklearn.metrics import confusion_matrix, roc_curve, ConfusionMatrixDisplay\nimport plotly.express as px\nimport pandas as pd\nimport torch\n\ntorch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {torch_device}\")\n\ntorch.manual_seed(0);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/argrewrite-v-2-corpus-sentence-pairs/sentence_pairs.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"actual_rev_types = []\n\nfor i in range(len(data)):\n    revision_type = data.loc[i, \"revision_type\"]\n    if revision_type != \"neither\":\n        actual_rev_types.append(revision_type)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions","metadata":{}},{"cell_type":"code","source":"def get_completion_logprobs(prefix, completion):\n    with torch.no_grad():\n        completion_ids = tokenizer.encode(completion, return_tensors=\"pt\").to(torch_device)\n        completion_len = completion_ids.shape[1]\n\n        whole_phrase = prefix + completion\n        whole_phrase_ids = tokenizer.encode(whole_phrase, return_tensors=\"pt\").to(torch_device)\n        whole_phrase_logits = model(whole_phrase_ids).logits\n        whole_phrase_logprobs = torch.log_softmax(whole_phrase_logits[0], 1)\n\n        completion_logprobs = []\n        for i in range(-completion_len-1, -1):\n            token_id = whole_phrase_ids[0][i+1]\n            logprob = whole_phrase_logprobs[i][token_id]\n            completion_logprobs.append(logprob)\n\n    return completion_logprobs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_roc(actual_rev_types, scores, metric_label):\n    fpr, tpr, thresholds = roc_curve(actual_rev_types, scores, pos_label=\"content\")\n    # Plot code generated by ChatGPT:\n    # https://chat.openai.com/share/2cb2a8d8-7d8e-46bf-b9b3-560db72f3f49\n    roc_df = pd.DataFrame({\"fpr\": fpr, \"tpr\": tpr, \"threshold\": thresholds})\n    fig = px.line(roc_df, x=\"fpr\", y=\"tpr\",\n                  title=f\"ROC Curve for {metric_label}\",\n                  labels={\n                    \"fpr\": \"False Positive Rate\",\n                    \"tpr\": \"True Positive Rate\",\n                    \"threshold\": f\"{metric_label} Threshold\",\n                  },\n                  hover_data={\"threshold\"}) \n\n    # Add a diagonal line (random classifier baseline)\n    fig.add_scatter(x=[0, 1], y=[0, 1], mode='lines', line=dict(color='gray', dash='dash'), name='Random Classifier')\n\n    # Show the plot\n    fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load GPT-2","metadata":{}},{"cell_type":"code","source":"model_name = \"openai-community/gpt2\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=torch_device)\n\n# Add the EOS token as PAD token\nif model.generation_config.pad_token_id is None:\n    model.generation_config.pad_token_id = model.generation_config.eos_token_id\n\ntokenizer.decode([tokenizer.eos_token_id]);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prompt Engineering\n\n## Adding More Keyword Description\n\n### Appending Description\n\n### Prepending Description","metadata":{}},{"cell_type":"code","source":"revision_data_expl_prepend = []\n\nfor i in range(len(data)):\n    revision_type = data.loc[i, \"revision_type\"]\n    if revision_type != \"neither\":\n        old_sentence = data.loc[i, \"original_sentence\"]\n        new_sentence = data.loc[i, \"revised_sentence\"]\n        content_stmt = f\"Since it changes the meaning of the sentence significantly, the following revision from '{old_sentence}' to '{new_sentence}' \"\n        surface_stmt = f\"Since it only changes words without affecting the overall meaning, the following revision from '{old_sentence}' to '{new_sentence}' \"\n        revision_data_expl_prepend.append(\n            {\n                \"content_stmt\": content_stmt,\n                \"surface_stmt\": surface_stmt,\n                \"revision_type\": revision_type,\n            }\n        )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shortening_factor = 1\n\npreds = []\nlogprob_diffs = []\n\nlogprob_threshold = 0\n\nfor i in range(len(revision_data_expl_prepend)//shortening_factor):\n    content_prompt = revision_data_expl_prepend[i][\"content_stmt\"]\n    surface_prompt = revision_data_expl_prepend[i][\"surface_stmt\"]\n\n    content_logprobs = torch.stack(get_completion_logprobs(prefix=content_prompt, completion=\"substantive\")).to(torch_device)\n    surface_logprobs = torch.stack(get_completion_logprobs(prefix=surface_prompt, completion=\"superficial\")).to(torch_device)\n\n    logprob_diff = (torch.sum(content_logprobs) - torch.sum(surface_logprobs)).item()\n    logprob_diffs.append(logprob_diff)\n\n    if logprob_diff > logprob_threshold:\n        preds.append(\"content\")\n    else:\n        preds.append(\"surface\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc(actual_rev_types[:len(prompts)//shortening_factor], logprob_diffs, metric_label=\"Logprob Diff\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(actual_rev_types[:len(prompts)//shortening_factor], preds, labels=[\"content\", \"surface\"])\ndisp = ConfusionMatrixDisplay(\n    confusion_matrix=cm, display_labels=[\"content\", \"surface\"]\n)\ndisp.plot();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## In-Context Learning","metadata":{}},{"cell_type":"code","source":"content_prompt = \"\"\"\nThe following revision from: As humans we rely currently on computers and technology for many of our daily tasks, and self-driving cars are an extension of the innovative culture our tech industries have already embraced.\nto: As humans we currently rely on computers and technology for many of our daily tasks, and self-driving cars are an extension of the innovative and tech driven culture our society has already embraced.\nis a content-level revision.\n\nThe following revision from: On the other hand, the car companies, your lawyers and some other groups will love this idea to death.\nto: On the other hand, the self- driving car companies, your lawyers and Google (they provide GPS) will love this idea to death.\"\nis a content-level revision.\n\nThe following revision from: Having these types of vehicles is also not worth taking away peopleâ€™s jobs and putting their do not have the technology to operate at a high level of safety in certain weather conditions.\nto: Having these types of vehicles is also not worth putting people's lives at risk, especially for those who live in areas where it snows and rains a lot, because these vehicles do not have the technology to operate at a high level of safety in those weather conditions.\nis a content-level revision.\n\nThe following revision from: {old_sentence}\nto: {new_sentence}\"\nis a \n\"\"\"\n\nsurface_prompt = \"\"\"\nThe following revision from: In light of recent events with the death of an Arizona woman at the hands of a self-driving Uber, many are unsure of what stance to take on the matter.\nto: In light of recent events with the death of an Arizona woman at the hands of a self-driving Uber, many are conflicted on what stance to take on the matter.\nis a surface-level revision.\n\nThe following revision from: The number of supporters and opponents will be equally the same.\nto: The number of supporters and opponents of this invention will be roughly the same.\nis a surface-level revision.\n\nThe following revision from: There are many variables to consider when thinking about individuals using self-driving cars: the weather, other traditional cars and their drivers, and the possibility of inappropriate - or developmentally inappropriate person - like children, mistakenly getting behind the wheel.\nto: There are many confounding variables to consider when thinking about individuals using self-driving cars: the weather, other traditional cars and their drivers, and the possibility of inappropriate - or developmentally-inappropriate persons - like children, mistakenly climbing behind the wheel.\nis a surface-level revision.\n\nThe following revision from: {old_sentence}\nto: {new_sentence}\nis a \n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"revision_data_ctx_example = []\nfor i in range(len(data)):\n    revision_type = data.loc[i, \"revision_type\"]\n    if revision_type != \"neither\":\n        old_sentence = data.loc[i, \"original_sentence\"]\n        new_sentence = data.loc[i, \"revised_sentence\"]\n        content_stmt = content_prompt.format(old_sentence=old_sentence, new_sentence=new_sentence)\n        surface_stmt = surface_prompt.format(old_sentence=old_sentence, new_sentence=new_sentence)\n        revision_data_ctx_example.append(\n            {\n                \"content_stmt\": content_stmt,\n                \"surface_stmt\": surface_stmt,\n                \"revision_type\": revision_type,\n            }\n        )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shortening_factor = 1\n\npreds = []\nlogprob_diffs = []\n\nlogprob_threshold = 0\n\nfor i in range(len(revision_data_expl_prepend)//shortening_factor):\n    content_prompt = revision_data_expl_prepend[i][\"content_stmt\"]\n    surface_prompt = revision_data_expl_prepend[i][\"surface_stmt\"]\n\n    content_logprobs = torch.stack(get_completion_logprobs(prefix=content_prompt, completion=\"substantive\")).to(torch_device)\n    surface_logprobs = torch.stack(get_completion_logprobs(prefix=surface_prompt, completion=\"superficial\")).to(torch_device)\n\n    logprob_diff = (torch.sum(content_logprobs) - torch.sum(surface_logprobs)).item()\n    logprob_diffs.append(logprob_diff)\n\n    if logprob_diff > logprob_threshold:\n        preds.append(\"content\")\n    else:\n        preds.append(\"surface\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc(actual_rev_types[:len(prompts)//shortening_factor], logprob_diffs, metric_label=\"Logprob Diff\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(actual_rev_types[:len(prompts)//shortening_factor], preds, labels=[\"content\", \"surface\"])\ndisp = ConfusionMatrixDisplay(\n    confusion_matrix=cm, display_labels=[\"content\", \"surface\"]\n)\ndisp.plot();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comparison Between GPT-2 and Gemma","metadata":{}},{"cell_type":"markdown","source":"## GPT-2 Baseline Classification","metadata":{}},{"cell_type":"code","source":"old_sentences = []\nnew_sentences = []\nprompts = []\n\nfor i in range(len(data)):\n    revision_type = data.loc[i, \"revision_type\"]\n    if revision_type != \"neither\":\n        old_sentence = data.loc[i, \"original_sentence\"]\n        old_sentences.append(old_sentence)\n        new_sentence = data.loc[i, \"revised_sentence\"]\n        new_sentences.append(new_sentence)\n        prompt = f\"The following revision from: \\n{old_sentence}\\nto:\\n{new_sentence}\\n is \"\n        prompts.append(prompt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shortening_factor = 1\n\npreds = []\nlogprob_diffs = []\n\nlogprob_threshold = 0\n\nfor i in range(len(prompts)//shortening_factor):\n    prompt = prompts[i]\n\n    content_logprobs = torch.stack(get_completion_logprobs(prefix=prompt, completion=\"substantive\")).to(torch_device)\n    surface_logprobs = torch.stack(get_completion_logprobs(prefix=prompt, completion=\"superficial\")).to(torch_device)\n\n    logprob_diff = (torch.sum(content_logprobs) - torch.sum(surface_logprobs)).item()\n    logprob_diffs.append(logprob_diff)\n\n    if logprob_diff > logprob_threshold:\n        preds.append(\"content\")\n    else:\n        preds.append(\"surface\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc(actual_rev_types[:len(prompts)//shortening_factor], logprob_diffs, metric_label=\"Logprob Diff\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(actual_rev_types[:len(prompts)//shortening_factor], preds, labels=[\"content\", \"surface\"])\ndisp = ConfusionMatrixDisplay(\n    confusion_matrix=cm, display_labels=[\"content\", \"surface\"]\n)\ndisp.plot();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gemma Baseline Classification","metadata":{}},{"cell_type":"code","source":"%pip install -U bitsandbytes\n%pip install -U accelerate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = \"/kaggle/input/gemma/transformers/2b/2\"\n    \nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n    \ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=torch_device, quantization_config=quantization_config)\n\ntokenizer.decode([tokenizer.eos_token_id]);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shortening_factor = 1\n\npreds = []\nlogprob_diffs = []\n\nlogprob_threshold = 0\n\nfor i in range(len(prompts)//shortening_factor):\n    prompt = prompts[i]\n\n    content_logprobs = torch.stack(get_completion_logprobs(prefix=prompt, completion=\"substantive\")).to(torch_device)\n    surface_logprobs = torch.stack(get_completion_logprobs(prefix=prompt, completion=\"superficial\")).to(torch_device)\n\n    logprob_diff = (torch.sum(content_logprobs) - torch.sum(surface_logprobs)).item()\n    logprob_diffs.append(logprob_diff)\n\n    if logprob_diff > logprob_threshold:\n        preds.append(\"content\")\n    else:\n        preds.append(\"surface\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc(actual_rev_types[:len(prompts)//shortening_factor], logprob_diffs, metric_label=\"Logprob Diff\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(actual_rev_types[:len(prompts)//shortening_factor], preds, labels=[\"content\", \"surface\"])\ndisp = ConfusionMatrixDisplay(\n    confusion_matrix=cm, display_labels=[\"content\", \"surface\"]\n)\ndisp.plot();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sbert","metadata":{}},{"cell_type":"markdown","source":"# Results","metadata":{}},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"# Appendix","metadata":{}}]}