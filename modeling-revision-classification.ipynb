{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8022240,"sourceType":"datasetVersion","datasetId":4727324},{"sourceId":8256074,"sourceType":"datasetVersion","datasetId":4899555},{"sourceId":11384,"sourceType":"modelInstanceVersion","modelInstanceId":6216}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nghtctrl/modeling-revision-classification?scriptVersionId=174675650\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Modeling Revision Classification\n\nDaniel Kim, Jason G. Chew, Jiho Kim\n\n# Introduction","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"%pip install sentence_transformers","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:22:56.207425Z","iopub.execute_input":"2024-04-29T05:22:56.207821Z","iopub.status.idle":"2024-04-29T05:23:08.511284Z","shell.execute_reply.started":"2024-04-29T05:22:56.207788Z","shell.execute_reply":"2024-04-29T05:23:08.510091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer, util\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom sklearn.metrics import confusion_matrix, roc_curve, ConfusionMatrixDisplay\nimport json\nimport plotly.express as px\nimport pandas as pd\nimport torch\n\ntorch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {torch_device}\")\n\ntorch.manual_seed(0);","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:23:20.91152Z","iopub.execute_input":"2024-04-29T05:23:20.91249Z","iopub.status.idle":"2024-04-29T05:23:24.88353Z","shell.execute_reply.started":"2024-04-29T05:23:20.912448Z","shell.execute_reply":"2024-04-29T05:23:24.882555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/argrewrite-v-2-corpus-sentence-pairs/sentence_pairs.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:24:13.42216Z","iopub.execute_input":"2024-04-29T05:24:13.422776Z","iopub.status.idle":"2024-04-29T05:24:13.457266Z","shell.execute_reply.started":"2024-04-29T05:24:13.422742Z","shell.execute_reply":"2024-04-29T05:24:13.456536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"actual_rev_types = []\n\nfor i in range(len(data)):\n    revision_type = data.loc[i, \"revision_type\"]\n    if revision_type != \"neither\":\n        actual_rev_types.append(revision_type)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:24:15.806133Z","iopub.execute_input":"2024-04-29T05:24:15.80651Z","iopub.status.idle":"2024-04-29T05:24:15.855461Z","shell.execute_reply.started":"2024-04-29T05:24:15.806479Z","shell.execute_reply":"2024-04-29T05:24:15.854744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions","metadata":{}},{"cell_type":"code","source":"def get_completion_logprobs(prefix, completion):\n    with torch.no_grad():\n        completion_ids = tokenizer.encode(completion, return_tensors=\"pt\").to(torch_device)\n        completion_len = completion_ids.shape[1]\n\n        whole_phrase = prefix + completion\n        whole_phrase_ids = tokenizer.encode(whole_phrase, return_tensors=\"pt\").to(torch_device)\n        whole_phrase_logits = model(whole_phrase_ids).logits\n        whole_phrase_logprobs = torch.log_softmax(whole_phrase_logits[0], 1)\n\n        completion_logprobs = []\n        for i in range(-completion_len-1, -1):\n            token_id = whole_phrase_ids[0][i+1]\n            logprob = whole_phrase_logprobs[i][token_id]\n            completion_logprobs.append(logprob)\n\n    return completion_logprobs","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:24:18.343376Z","iopub.execute_input":"2024-04-29T05:24:18.34374Z","iopub.status.idle":"2024-04-29T05:24:18.350941Z","shell.execute_reply.started":"2024-04-29T05:24:18.343709Z","shell.execute_reply":"2024-04-29T05:24:18.350038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_roc(actual_rev_types, scores, metric_label):\n    fpr, tpr, thresholds = roc_curve(actual_rev_types, scores, pos_label=\"content\")\n    # Plot code generated by ChatGPT:\n    # https://chat.openai.com/share/2cb2a8d8-7d8e-46bf-b9b3-560db72f3f49\n    roc_df = pd.DataFrame({\"fpr\": fpr, \"tpr\": tpr, \"threshold\": thresholds})\n    fig = px.line(roc_df, x=\"fpr\", y=\"tpr\",\n                  title=f\"ROC Curve for {metric_label}\",\n                  labels={\n                    \"fpr\": \"False Positive Rate\",\n                    \"tpr\": \"True Positive Rate\",\n                    \"threshold\": f\"{metric_label} Threshold\",\n                  },\n                  hover_data={\"threshold\"}) \n\n    # Add a diagonal line (random classifier baseline)\n    fig.add_scatter(x=[0, 1], y=[0, 1], mode='lines', line=dict(color='gray', dash='dash'), name='Random Classifier')\n\n    # Show the plot\n    fig.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:24:32.578608Z","iopub.execute_input":"2024-04-29T05:24:32.578981Z","iopub.status.idle":"2024-04-29T05:24:32.585837Z","shell.execute_reply.started":"2024-04-29T05:24:32.578955Z","shell.execute_reply":"2024-04-29T05:24:32.584919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## Load GPT-2","metadata":{}},{"cell_type":"code","source":"model_name = \"openai-community/gpt2\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=torch_device)\n\n# Add the EOS token as PAD token\nif model.generation_config.pad_token_id is None:\n    model.generation_config.pad_token_id = model.generation_config.eos_token_id\n\ntokenizer.decode([tokenizer.eos_token_id]);","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:24:36.137995Z","iopub.execute_input":"2024-04-29T05:24:36.138644Z","iopub.status.idle":"2024-04-29T05:24:57.473241Z","shell.execute_reply.started":"2024-04-29T05:24:36.138613Z","shell.execute_reply":"2024-04-29T05:24:57.472418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## GPT-2 Baseline Classification","metadata":{}},{"cell_type":"code","source":"old_sentences = []\nnew_sentences = []\nprompts = []\n\nfor i in range(len(data)):\n    revision_type = data.loc[i, \"revision_type\"]\n    if revision_type != \"neither\":\n        old_sentence = data.loc[i, \"original_sentence\"]\n        new_sentence = data.loc[i, \"revised_sentence\"]\n        prompt = f\"The following revision from: \\n{old_sentence}\\nto:\\n{new_sentence}\\n \"\n        prompts.append(prompt)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:25:53.858887Z","iopub.execute_input":"2024-04-29T05:25:53.859606Z","iopub.status.idle":"2024-04-29T05:25:53.952208Z","shell.execute_reply.started":"2024-04-29T05:25:53.859576Z","shell.execute_reply":"2024-04-29T05:25:53.951456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shortening_factor = 1\n\ngpt2_preds = []\ngpt2_logprob_diffs = []\n\nlogprob_threshold = 0\n\nfor i in range(len(prompts)//shortening_factor):\n    prompt = prompts[i]\n\n    content_logprobs = torch.stack(get_completion_logprobs(prefix=prompt, completion=\"is substantive\")).to(torch_device)\n    surface_logprobs = torch.stack(get_completion_logprobs(prefix=prompt, completion=\"is superficial\")).to(torch_device)\n\n    logprob_diff = (torch.sum(content_logprobs) - torch.sum(surface_logprobs)).item()\n    gpt2_logprob_diffs.append(logprob_diff)\n\n    if logprob_diff > logprob_threshold:\n        gpt2_preds.append(\"content\")\n    else:\n        gpt2_preds.append(\"surface\")","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:26:05.039073Z","iopub.execute_input":"2024-04-29T05:26:05.039925Z","iopub.status.idle":"2024-04-29T05:26:39.66237Z","shell.execute_reply.started":"2024-04-29T05:26:05.039893Z","shell.execute_reply":"2024-04-29T05:26:39.661318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc(actual_rev_types[:len(prompts)//shortening_factor], gpt2_logprob_diffs, metric_label=\"Logprob Diff\")","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:26:59.221958Z","iopub.execute_input":"2024-04-29T05:26:59.22266Z","iopub.status.idle":"2024-04-29T05:26:59.894182Z","shell.execute_reply.started":"2024-04-29T05:26:59.222623Z","shell.execute_reply":"2024-04-29T05:26:59.893329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(actual_rev_types[:len(prompts)//shortening_factor], gpt2_preds, labels=[\"content\", \"surface\"])\ndisp = ConfusionMatrixDisplay(\n    confusion_matrix=cm, display_labels=[\"content\", \"surface\"]\n)\ndisp.plot();","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:27:02.860074Z","iopub.execute_input":"2024-04-29T05:27:02.860457Z","iopub.status.idle":"2024-04-29T05:27:03.226265Z","shell.execute_reply.started":"2024-04-29T05:27:02.860426Z","shell.execute_reply":"2024-04-29T05:27:03.225094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prompt Engineering\n\n## Adding More Keyword Description\n\n### Appending Description\n- We are not considering the cases for appending the explanation to the revision data. Our current approach is to use the last word as a type of revision, meaning that adding explanations later would not have any attention to it.\n\n### Prepending Description\n- We prepended the descriptions for the keywords for content-level revision and surface-level revision.","metadata":{}},{"cell_type":"code","source":"descriptive_prompts = []\n\nfor i in range(len(data)):\n    revision_type = data.loc[i, \"revision_type\"]\n    if revision_type != \"neither\":\n        old_sentence = data.loc[i, \"original_sentence\"]\n        new_sentence = data.loc[i, \"revised_sentence\"]\n        content_stmt = f\"Substantive revisions change the meaning significantly, so the following revision from '{old_sentence}' to '{new_sentence}' \"\n        surface_stmt = f\"Superficial revisions only change words without affecting the overall meaning, so the following revision from '{old_sentence}' to '{new_sentence}' \"\n        descriptive_prompts.append(\n            {\n                \"content_stmt\": content_stmt,\n                \"surface_stmt\": surface_stmt,\n            }\n        )","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:27:10.610354Z","iopub.execute_input":"2024-04-29T05:27:10.611313Z","iopub.status.idle":"2024-04-29T05:27:10.702589Z","shell.execute_reply.started":"2024-04-29T05:27:10.611275Z","shell.execute_reply":"2024-04-29T05:27:10.701901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shortening_factor = 1\n\npreds = []\nlogprob_diffs = []\n\nlogprob_threshold = 0\n\nfor i in range(len(descriptive_prompts)//shortening_factor):\n    content_prompt = descriptive_prompts[i][\"content_stmt\"]\n    surface_prompt = descriptive_prompts[i][\"surface_stmt\"]\n\n    content_logprobs = torch.stack(get_completion_logprobs(prefix=content_prompt, completion=\"is substantive\")).to(torch_device)\n    surface_logprobs = torch.stack(get_completion_logprobs(prefix=surface_prompt, completion=\"is superficial\")).to(torch_device)\n\n    logprob_diff = (torch.sum(content_logprobs) - torch.sum(surface_logprobs)).item()\n    logprob_diffs.append(logprob_diff)\n\n    if logprob_diff > logprob_threshold:\n        preds.append(\"content\")\n    else:\n        preds.append(\"surface\")","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:27:32.745839Z","iopub.execute_input":"2024-04-29T05:27:32.746247Z","iopub.status.idle":"2024-04-29T05:28:06.482587Z","shell.execute_reply.started":"2024-04-29T05:27:32.74622Z","shell.execute_reply":"2024-04-29T05:28:06.481777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc(actual_rev_types[:len(descriptive_prompts)//shortening_factor], logprob_diffs, metric_label=\"Logprob Diff\")","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:28:16.743479Z","iopub.execute_input":"2024-04-29T05:28:16.744264Z","iopub.status.idle":"2024-04-29T05:28:16.814677Z","shell.execute_reply.started":"2024-04-29T05:28:16.744227Z","shell.execute_reply":"2024-04-29T05:28:16.813775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(actual_rev_types[:len(descriptive_prompts)//shortening_factor], preds, labels=[\"content\", \"surface\"])\ndisp = ConfusionMatrixDisplay(\n    confusion_matrix=cm, display_labels=[\"content\", \"surface\"]\n)\ndisp.plot();","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:28:31.261539Z","iopub.execute_input":"2024-04-29T05:28:31.262253Z","iopub.status.idle":"2024-04-29T05:28:31.57484Z","shell.execute_reply.started":"2024-04-29T05:28:31.262215Z","shell.execute_reply":"2024-04-29T05:28:31.573858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analysis\nPrepending the description for the keywords for content-level revision and surface-level revision did improve the performance of the model from the baseline, according to the ROC curve. However, the improvement was not significant.","metadata":{}},{"cell_type":"markdown","source":"## Few-Shot Prompting\n- For In-Context Learning, I have provided relevant examples of revisions\n- Example sentence pairs are randomly chosen from 2to3 revisions, which are not seen by the model\n- We will try both two seperate prompts (for revision types) as well as one composite prompt that contains both surface and content-level revisions","metadata":{}},{"cell_type":"code","source":"few_shot_prompt = \"\"\"\nThe following revision from: Having these types of vehicles is also not worth taking away peopleâ€™s jobs and putting their do not have the technology to operate at a high level of safety in certain weather conditions.\nto: Having these types of vehicles is also not worth putting people's lives at risk, especially for those who live in areas where it snows and rains a lot, because these vehicles do not have the technology to operate at a high level of safety in those weather conditions.\nis substantive.\n\nThe following revision from: In light of recent events with the death of an Arizona woman at the hands of a self-driving Uber, many are unsure of what stance to take on the matter.\nto: In light of recent events with the death of an Arizona woman at the hands of a self-driving Uber, many are conflicted on what stance to take on the matter.\nis superficial.\n\nThe following revision from: On the other hand, the car companies, your lawyers and some other groups will love this idea to death.\nto: On the other hand, the self- driving car companies, your lawyers and Google (they provide GPS) will love this idea to death.\"\nis substantive.\n\nThe following revision from: There are many variables to consider when thinking about individuals using self-driving cars: the weather, other traditional cars and their drivers, and the possibility of inappropriate - or developmentally inappropriate person - like children, mistakenly getting behind the wheel.\nto: There are many confounding variables to consider when thinking about individuals using self-driving cars: the weather, other traditional cars and their drivers, and the possibility of inappropriate - or developmentally-inappropriate persons - like children, mistakenly climbing behind the wheel.\nis superficial.\n\nThe following revision from: {old_sentence}\nto: {new_sentence}\nis \n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:29:19.907191Z","iopub.execute_input":"2024-04-29T05:29:19.907895Z","iopub.status.idle":"2024-04-29T05:29:19.914586Z","shell.execute_reply.started":"2024-04-29T05:29:19.907855Z","shell.execute_reply":"2024-04-29T05:29:19.91354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompts = []\nfor i in range(len(data)):\n    revision_type = data.loc[i, \"revision_type\"]\n    if revision_type != \"neither\":\n        old_sentence = data.loc[i, \"original_sentence\"]\n        new_sentence = data.loc[i, \"revised_sentence\"]\n        composite_stmts = few_shot_prompt.format(old_sentence=old_sentence, new_sentence=new_sentence)\n        prompts.append(composite_stmts)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:29:38.302786Z","iopub.execute_input":"2024-04-29T05:29:38.303165Z","iopub.status.idle":"2024-04-29T05:29:38.404159Z","shell.execute_reply.started":"2024-04-29T05:29:38.303132Z","shell.execute_reply":"2024-04-29T05:29:38.403192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shortening_factor = 1\n\npreds = []\nlogprob_diffs = []\n\nlogprob_threshold = 0\n\nfor i in range(len(prompts)//shortening_factor):\n    prompt = prompts[i]\n\n    content_logprobs = torch.stack(get_completion_logprobs(prefix=prompt, completion=\"substantive\")).to(torch_device)\n    surface_logprobs = torch.stack(get_completion_logprobs(prefix=prompt, completion=\"superficial\")).to(torch_device)\n\n    logprob_diff = (torch.sum(content_logprobs) - torch.sum(surface_logprobs)).item()\n    logprob_diffs.append(logprob_diff)\n\n    if logprob_diff > logprob_threshold:\n        preds.append(\"content\")\n    else:\n        preds.append(\"surface\")","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:29:40.319872Z","iopub.execute_input":"2024-04-29T05:29:40.320239Z","iopub.status.idle":"2024-04-29T05:31:43.158442Z","shell.execute_reply.started":"2024-04-29T05:29:40.320212Z","shell.execute_reply":"2024-04-29T05:31:43.157589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc(actual_rev_types[:len(prompts)//shortening_factor], logprob_diffs, metric_label=\"Logprob Diff\")","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:31:47.03786Z","iopub.execute_input":"2024-04-29T05:31:47.038235Z","iopub.status.idle":"2024-04-29T05:31:47.105538Z","shell.execute_reply.started":"2024-04-29T05:31:47.038206Z","shell.execute_reply":"2024-04-29T05:31:47.104656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(actual_rev_types[:len(prompts)//shortening_factor], preds, labels=[\"content\", \"surface\"])\ndisp = ConfusionMatrixDisplay(\n    confusion_matrix=cm, display_labels=[\"content\", \"surface\"]\n)\ndisp.plot();","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:32:20.397972Z","iopub.execute_input":"2024-04-29T05:32:20.398937Z","iopub.status.idle":"2024-04-29T05:32:20.701299Z","shell.execute_reply.started":"2024-04-29T05:32:20.398899Z","shell.execute_reply":"2024-04-29T05:32:20.700354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analysis\nLearning from the in-context examples seems to be effective in improving the model's performance. The model was able to correctly classify the revision types with a higher accuracy when it was trained with in-context examples. The composite prompt seems to be more effective in improving the model's performance than the two separate prompts, since it gives the change for model to learn the difference between the two revision types.","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Comparison Between GPT-2 and Gemma","metadata":{}},{"cell_type":"markdown","source":"## Recall: GPT-2 Baseline Classification","metadata":{}},{"cell_type":"code","source":"plot_roc(actual_rev_types[:len(prompts)//shortening_factor], gpt2_logprob_diffs, metric_label=\"Logprob Diff\")","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:32:45.116874Z","iopub.execute_input":"2024-04-29T05:32:45.117244Z","iopub.status.idle":"2024-04-29T05:32:45.183596Z","shell.execute_reply.started":"2024-04-29T05:32:45.117209Z","shell.execute_reply":"2024-04-29T05:32:45.182639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(actual_rev_types[:len(prompts)//shortening_factor], gpt2_preds, labels=[\"content\", \"surface\"])\ndisp = ConfusionMatrixDisplay(\n    confusion_matrix=cm, display_labels=[\"content\", \"surface\"]\n)\ndisp.plot();","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:32:47.471248Z","iopub.execute_input":"2024-04-29T05:32:47.471668Z","iopub.status.idle":"2024-04-29T05:32:47.776688Z","shell.execute_reply.started":"2024-04-29T05:32:47.471634Z","shell.execute_reply":"2024-04-29T05:32:47.775854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gemma Baseline Classification","metadata":{}},{"cell_type":"code","source":"%pip install -U bitsandbytes\n%pip install accelerate","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = \"/kaggle/input/gemma/transformers/2b/2\"\n    \nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n    \ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=torch_device, quantization_config=quantization_config)\n\ntokenizer.decode([tokenizer.eos_token_id]);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shortening_factor = 1\n\npreds = []\nlogprob_diffs = []\n\nlogprob_threshold = 0\n\nfor i in range(len(prompts)//shortening_factor):\n    prompt = prompts[i]\n\n    content_logprobs = torch.stack(get_completion_logprobs(prefix=prompt, completion=\"is substantive\")).to(torch_device)\n    surface_logprobs = torch.stack(get_completion_logprobs(prefix=prompt, completion=\"is superficial\")).to(torch_device)\n\n    logprob_diff = (torch.sum(content_logprobs) - torch.sum(surface_logprobs)).item()\n    logprob_diffs.append(logprob_diff)\n\n    if logprob_diff > logprob_threshold:\n        preds.append(\"content\")\n    else:\n        preds.append(\"surface\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"/kaggle/input/gemma-data/gemma_data.json\", \"r\") as file:\n    gemma_data = json.load(file)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:32:53.676865Z","iopub.execute_input":"2024-04-29T05:32:53.677744Z","iopub.status.idle":"2024-04-29T05:32:53.691759Z","shell.execute_reply.started":"2024-04-29T05:32:53.677686Z","shell.execute_reply":"2024-04-29T05:32:53.690672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc(actual_rev_types[:len(prompts)], gemma_data[\"logprob_diffs\"], metric_label=\"Logprob Diff\")","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:32:56.186307Z","iopub.execute_input":"2024-04-29T05:32:56.187023Z","iopub.status.idle":"2024-04-29T05:32:56.251917Z","shell.execute_reply.started":"2024-04-29T05:32:56.18699Z","shell.execute_reply":"2024-04-29T05:32:56.251098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(actual_rev_types[:len(prompts)], gemma_data[\"predictions\"], labels=[\"content\", \"surface\"])\ndisp = ConfusionMatrixDisplay(\n    confusion_matrix=cm, display_labels=[\"content\", \"surface\"]\n)\ndisp.plot();","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:33:07.073432Z","iopub.execute_input":"2024-04-29T05:33:07.073826Z","iopub.status.idle":"2024-04-29T05:33:07.406777Z","shell.execute_reply.started":"2024-04-29T05:33:07.073793Z","shell.execute_reply":"2024-04-29T05:33:07.405752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SBERT","metadata":{}},{"cell_type":"code","source":"old_sentences = []\nnew_sentences = []\nactual_rev_types = []\n\nfor i in range(len(data)):\n    revision_type = data.loc[i, \"revision_type\"]\n    if revision_type != \"neither\":\n        old_sentence = str(data.loc[i, \"original_sentence\"])\n        old_sentences.append(old_sentence)\n        new_sentence = str(data.loc[i, \"revised_sentence\"])\n        new_sentences.append(new_sentence)\n        actual_rev_types.append(revision_type)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:33:10.949106Z","iopub.execute_input":"2024-04-29T05:33:10.949478Z","iopub.status.idle":"2024-04-29T05:33:11.048287Z","shell.execute_reply.started":"2024-04-29T05:33:10.94945Z","shell.execute_reply":"2024-04-29T05:33:11.046931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = SentenceTransformer(\"all-mpnet-base-v2\").to(torch_device)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:33:20.185995Z","iopub.execute_input":"2024-04-29T05:33:20.186865Z","iopub.status.idle":"2024-04-29T05:33:25.277959Z","shell.execute_reply.started":"2024-04-29T05:33:20.186828Z","shell.execute_reply":"2024-04-29T05:33:25.277013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shortening_factor = 1\npreds = []\ndiff_scores = []\n\ndiff_threshold = -0.661\n\nfor i in range(len(old_sentences)//shortening_factor):\n    \n    # Cosine-similarity code adapted from: https://www.sbert.net/docs/usage/semantic_textual_similarity.html\n    with torch.no_grad():\n        # Compute embeddings\n        original_embed = model.encode(old_sentences[i], convert_to_tensor=True, show_progress_bar=False).to(torch_device)\n        revision_embed = model.encode(new_sentences[i], convert_to_tensor=True, show_progress_bar=False).to(torch_device)\n\n        # Compute cosine-similarities\n        cos_similarity = util.cos_sim(original_embed, revision_embed)\n        diff_score = -cos_similarity[0].item()\n        diff_scores.append(diff_score)\n\n        if diff_score > diff_threshold:\n            preds.append(\"content\")\n        else:\n            preds.append(\"surface\")","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:43:49.625894Z","iopub.execute_input":"2024-04-29T05:43:49.626767Z","iopub.status.idle":"2024-04-29T05:44:27.500061Z","shell.execute_reply.started":"2024-04-29T05:43:49.626731Z","shell.execute_reply":"2024-04-29T05:44:27.499176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_roc(actual_rev_types[:len(old_sentences)//shortening_factor], diff_scores, metric_label=\"Semantic Diff\")","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:44:31.416929Z","iopub.execute_input":"2024-04-29T05:44:31.417343Z","iopub.status.idle":"2024-04-29T05:44:31.492656Z","shell.execute_reply.started":"2024-04-29T05:44:31.41731Z","shell.execute_reply":"2024-04-29T05:44:31.491741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(actual_rev_types[:len(old_sentences)//shortening_factor], preds, labels=[\"content\", \"surface\"])\ndisp = ConfusionMatrixDisplay(\n    confusion_matrix=cm, display_labels=[\"content\", \"surface\"]\n)\ndisp.plot();","metadata":{"execution":{"iopub.status.busy":"2024-04-29T05:44:35.055282Z","iopub.execute_input":"2024-04-29T05:44:35.056255Z","iopub.status.idle":"2024-04-29T05:44:35.343805Z","shell.execute_reply.started":"2024-04-29T05:44:35.056216Z","shell.execute_reply":"2024-04-29T05:44:35.342739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results","metadata":{}},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"# Appendix","metadata":{}}]}