{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8022240,"sourceType":"datasetVersion","datasetId":4727324},{"sourceId":8256074,"sourceType":"datasetVersion","datasetId":4899555},{"sourceId":11384,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":6216}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nghtctrl/modeling-revision-classification?scriptVersionId=174974139\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Modeling Revision Classification\n\nDaniel Kim\\*, Jason G. Chew\\*, Jiho Kim\\*\n\n*Equal Contribution","metadata":{}},{"cell_type":"markdown","source":"# Introduction\nIn the writing process, effective textual revisions typically result in expansive alterations to the semantic content of a text, as opposed to prescriptive alterations like proofreading; however, novice writers often tend to favor the latter ([Flower et. al 1986](https://doi.org/10.2307/357381)). Therefore, the identification of major semantic alterations in the revisions of student writers could be used to benchmark their progress towards more effective revision. \n\nTo this end, the following report analyzes various applications of language models towards revision evaluation, which is in this case framed as a binary classification task for identifying revisions as “content” (substantive) or “surface” (superficial) revisions. \n\nThis report considers two approaches to the binary classification task.\n\n1. Completion prompting: when given a “fill-in-the-blank” classification prompt like “...the revision is _________”, a language model can implicitly make predictions thanks to the logprobs, or “likelihoods,” it computes for each of the two possible classification terms (“substantive” and “superficial”). Once the model computes these likelihoods, they may be compared with one another to make the classification.\n\n2. Similarity scores: a language model can compute abstract representations (embeddings) of an original and revised text based on their semantic content, and the similarity of those semantic embeddings can be used as a measure of how little a revision changed the “content” of a sentence; these similarities can then be used to predict whether a revision alters a sentence’s semantic meaning significantly enough to be considered a “content” revision.\n\nWithin approach 1, performance on the task improved slightly by preprending contextual information to the prompt, such as example classifications and definitions of the term.","metadata":{}},{"cell_type":"markdown","source":"# Description of the Dataset\nWe use a dataset called “[ArgRewriteV2](https://argrewrite.cs.pitt.edu/)”, which contains essays written by students in response to a single prompt about the implications of self-driving cars. Each essay has three versions: the original draft, a revision, and a second revision. The second revisions were not made under experimentally constant circumstances, so we will only use the first revisions for our evaluation. \n\nThe dataset contains essay-level, sentence-level, and subsentence-level data. This report only uses the sentence-level data. Our goal is to classify the revision types of revisions using two categories: “superficial” and “substantive.” The “superficial” category corresponds to the revision categories “Word Usage” and “Conventions/Grammar/Spelling” in the dataset. The “substantive” category corresponds to the categories \"Claim/Ideas\", \"Organization\", \"Warrant/Reasoning/Backing\", \"Rebuttal/Reservation\", \"Precision\", \"General Content\", and \"Grammar\" in the dataset. (As opposed to the original dataset’s authors, we placed \"Organization\" in the “substantive” category as an arguably significant change to a sentence.)","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"%pip install sentence_transformers","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Import Necessary Modules","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer, util\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve, ConfusionMatrixDisplay\nimport json\nimport plotly.express as px\nimport pandas as pd\nimport torch\n\ntorch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {torch_device}\")\n\ntorch.manual_seed(0);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/argrewrite-v-2-corpus-sentence-pairs/sentence_pairs.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"actual_rev_types = []\n\nfor i in range(len(data)):\n    revision_type = data.loc[i, \"revision_type\"]\n    if revision_type != \"neither\":\n        actual_rev_types.append(revision_type)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions","metadata":{}},{"cell_type":"markdown","source":"##### Get the logprobs for complition given prefix","metadata":{}},{"cell_type":"code","source":"def get_completion_logprobs(prefix, completion):\n    with torch.no_grad():\n        completion_ids = tokenizer.encode(completion, return_tensors=\"pt\").to(torch_device)\n        completion_len = completion_ids.shape[1]\n\n        whole_phrase = prefix + completion\n        whole_phrase_ids = tokenizer.encode(whole_phrase, return_tensors=\"pt\").to(torch_device)\n        whole_phrase_logits = model(whole_phrase_ids).logits\n        whole_phrase_logprobs = torch.log_softmax(whole_phrase_logits[0], 1)\n\n        completion_logprobs = []\n        for i in range(-completion_len-1, -1):\n            token_id = whole_phrase_ids[0][i+1]\n            logprob = whole_phrase_logprobs[i][token_id]\n            completion_logprobs.append(logprob)\n\n    return completion_logprobs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Function for Plotting ROC Curve","metadata":{}},{"cell_type":"code","source":"def plot_roc(actual_rev_types, scores, metric_label):\n    fpr, tpr, thresholds = roc_curve(actual_rev_types, scores, pos_label=\"content\")\n    # Plot code generated by ChatGPT:\n    # https://chat.openai.com/share/2cb2a8d8-7d8e-46bf-b9b3-560db72f3f49\n    roc_df = pd.DataFrame({\"fpr\": fpr, \"tpr\": tpr, \"threshold\": thresholds})\n    fig = px.line(roc_df, x=\"fpr\", y=\"tpr\",\n                  title=f\"ROC Curve for {metric_label}\",\n                  labels={\n                    \"fpr\": \"False Positive Rate\",\n                    \"tpr\": \"True Positive Rate\",\n                    \"threshold\": \"Threshold\",\n                  },\n                  hover_data={\"threshold\"}) \n\n    # Add a diagonal line (random classifier baseline)\n    fig.add_scatter(x=[0, 1], y=[0, 1], mode='lines', line=dict(color='gray', dash='dash'), name='Random Classifier')\n\n    # Show the plot\n    fig.show()\n    \n    # Calculate area under the ROC curve\n    binary_rev_labels = [1 if label == \"content\" else 0 for label in actual_rev_types]\n    auc = roc_auc_score(binary_rev_labels, scores)\n    print(\"Area under the ROC Curve:\", auc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Completion Model 1: GPT-2 (Baseline)\nWe will use GPT-2 as our baseline model for the completion prompting approach. GPT-2 is an older model from 2019 which at the time of its publishing significantly furthered the possibility of “competent generalists” for NLP tasks beyond “narrow expert” systems ([Radford et. al 2019](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)). We use the smaller 137-million parameter version of this model to reduce the compute power and time required to make a prediction, as we will be making many predictions over the course of this report. As one of the first language models that marked significant progress towards “competent generalist” performance, GPT-2 is a fitting model to use as a baseline to determine how such language models might perform on our specific classification task. ","metadata":{}},{"cell_type":"markdown","source":"### Load GPT-2","metadata":{}},{"cell_type":"code","source":"model_name = \"openai-community/gpt2\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=torch_device)\n\n# Add the EOS token as PAD token\nif model.generation_config.pad_token_id is None:\n    model.generation_config.pad_token_id = model.generation_config.eos_token_id\n\ntokenizer.decode([tokenizer.eos_token_id]);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompts = []\n\nfor i in range(len(data)):\n    revision_type = data.loc[i, \"revision_type\"]\n    if revision_type != \"neither\":\n        old_sentence = data.loc[i, \"original_sentence\"]\n        new_sentence = data.loc[i, \"revised_sentence\"]\n        prompt = f\"The following revision from: \\n{old_sentence}\\nto:\\n{new_sentence}\\n \"\n        prompts.append(prompt)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Logprob threshold\nWe are using the threshold of 0 for the purpose of experimentation.","metadata":{}},{"cell_type":"code","source":"shortening_factor = 1\n\ngpt2_preds = []\ngpt2_logprob_diffs = []\n\nlogprob_threshold = 0\n\nfor i in range(len(prompts)//shortening_factor):\n    prompt = prompts[i]\n\n    content_logprobs = torch.stack(get_completion_logprobs(prefix=prompt, completion=\"is substantive\")).to(torch_device)\n    surface_logprobs = torch.stack(get_completion_logprobs(prefix=prompt, completion=\"is superficial\")).to(torch_device)\n\n    logprob_diff = (torch.sum(content_logprobs) - torch.sum(surface_logprobs)).item()\n    gpt2_logprob_diffs.append(logprob_diff)\n\n    if logprob_diff > logprob_threshold:\n        gpt2_preds.append(\"content\")\n    else:\n        gpt2_preds.append(\"surface\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### ROC Curve for GPT-2 Baseline","metadata":{}},{"cell_type":"code","source":"plot_roc(actual_rev_types[:len(prompts)//shortening_factor], gpt2_logprob_diffs, metric_label=\"Logprob Diff (GPT-2 Baseline)\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Confusion Matrix for GPT-2 Baseline","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(actual_rev_types[:len(prompts)//shortening_factor], gpt2_preds, labels=[\"content\", \"surface\"])\ndisp = ConfusionMatrixDisplay(\n    confusion_matrix=cm, display_labels=[\"content\", \"surface\"]\n)\ndisp.plot();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prompt Engineering Approach\n\n## Adding Reasoning for the Classification Terms to the Prefix\nWe originally used a longer completion that explained why a revision might or might not be “substantive.” However, this created many complicated token interdependencies that were difficult to separate from one another in our analysis, so we elected to calculate logprobs on completions which only varied the classification terms themselves.\n\n## Including Reasoning in the Prefix\nWe added reasoning in the prefix by defining “substantive” and “superficial” revisions before calculating the completion. \nThe prompts were defined as follows:\n- Defining “substantive” revision: \"Substantive revisions change the meaning significantly, so the following revision from '{old_sentence}' to '{new_sentence}' \"\n- Defining “superficial” revision: \"Superficial revisions only change words without affecting the overall meaning, so the following revision from '{old_sentence}' to '{new_sentence}' \"","metadata":{}},{"cell_type":"markdown","source":"##### Descriptions for keywords are added (prepended) to the prefix","metadata":{}},{"cell_type":"code","source":"descriptive_prompts = []\n\nfor i in range(len(data)):\n    revision_type = data.loc[i, \"revision_type\"]\n    if revision_type != \"neither\":\n        old_sentence = data.loc[i, \"original_sentence\"]\n        new_sentence = data.loc[i, \"revised_sentence\"]\n        content_stmt = f\"Substantive revisions change the meaning significantly, so the following revision from '{old_sentence}' to '{new_sentence}' \"\n        surface_stmt = f\"Superficial revisions only change words without affecting the overall meaning, so the following revision from '{old_sentence}' to '{new_sentence}' \"\n        descriptive_prompts.append(\n            {\n                \"content_stmt\": content_stmt,\n                \"surface_stmt\": surface_stmt,\n            }\n        )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Classify depending on the difference in logprobs ","metadata":{}},{"cell_type":"code","source":"shortening_factor = 1\n\ndescriptive_preds = []\ndescriptive_logprob_diffs = []\n\nlogprob_threshold = 0\n\nfor i in range(len(descriptive_prompts)//shortening_factor):\n    content_prompt = descriptive_prompts[i][\"content_stmt\"]\n    surface_prompt = descriptive_prompts[i][\"surface_stmt\"]\n\n    content_logprobs = torch.stack(get_completion_logprobs(prefix=content_prompt, completion=\"is substantive\")).to(torch_device)\n    surface_logprobs = torch.stack(get_completion_logprobs(prefix=surface_prompt, completion=\"is superficial\")).to(torch_device)\n\n    logprob_diff = (torch.sum(content_logprobs) - torch.sum(surface_logprobs)).item()\n    descriptive_logprob_diffs.append(logprob_diff)\n\n    if logprob_diff > logprob_threshold:\n        descriptive_preds.append(\"content\")\n    else:\n        descriptive_preds.append(\"surface\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### ROC Curve for GPT-2, Prepended Descriptions","metadata":{}},{"cell_type":"code","source":"plot_roc(actual_rev_types[:len(descriptive_prompts)//shortening_factor], descriptive_logprob_diffs, metric_label=\"Logprob Diff (GPT-2 Classification Description)\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Confusion Matrix for GPT-2, Prepended Descriptions","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(actual_rev_types[:len(descriptive_prompts)//shortening_factor], descriptive_preds, labels=[\"content\", \"surface\"])\ndisp = ConfusionMatrixDisplay(\n    confusion_matrix=cm, display_labels=[\"content\", \"surface\"]\n)\ndisp.plot();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analysis\nPrepending the description for the keywords for content-level revision and surface-level revision did improve the performance of the model from the baseline. This raised the area under the ROC curve a little, but the improvement was not significant.","metadata":{}},{"cell_type":"markdown","source":"## Few-Shot Prompting by Examples in the Prefix\nTo facilitate few-shot learning, the prompt prefix includes relevant examples of randomly chosen revision pairs and their classifications, in the format:\n- The following revision from: “<original1> to <revision1> is substantive.\n- The following revision from: “<original2> to <revision2> is superficial.\n- The following revision from: “<original3> to <revision3> is substantive.\n- The following revision from: “<original4> to <revision4> is superficial.\n- The following revision from: “<original5> to <revision5> is ","metadata":{}},{"cell_type":"markdown","source":"##### In-context Examples relevant to revision cases","metadata":{}},{"cell_type":"code","source":"few_shot_prompt = \"\"\"\nThe following revision from: Having these types of vehicles is also not worth taking away people’s jobs and putting their do not have the technology to operate at a high level of safety in certain weather conditions.\nto: Having these types of vehicles is also not worth putting people's lives at risk, especially for those who live in areas where it snows and rains a lot, because these vehicles do not have the technology to operate at a high level of safety in those weather conditions.\nis substantive.\n\nThe following revision from: In light of recent events with the death of an Arizona woman at the hands of a self-driving Uber, many are unsure of what stance to take on the matter.\nto: In light of recent events with the death of an Arizona woman at the hands of a self-driving Uber, many are conflicted on what stance to take on the matter.\nis superficial.\n\nThe following revision from: On the other hand, the car companies, your lawyers and some other groups will love this idea to death.\nto: On the other hand, the self- driving car companies, your lawyers and Google (they provide GPS) will love this idea to death.\"\nis substantive.\n\nThe following revision from: There are many variables to consider when thinking about individuals using self-driving cars: the weather, other traditional cars and their drivers, and the possibility of inappropriate - or developmentally inappropriate person - like children, mistakenly getting behind the wheel.\nto: There are many confounding variables to consider when thinking about individuals using self-driving cars: the weather, other traditional cars and their drivers, and the possibility of inappropriate - or developmentally-inappropriate persons - like children, mistakenly climbing behind the wheel.\nis superficial.\n\nThe following revision from: {old_sentence}\nto: {new_sentence} is \n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Concatenate prompts","metadata":{}},{"cell_type":"code","source":"few_shot_prompts = []\nfor i in range(len(data)):\n    revision_type = data.loc[i, \"revision_type\"]\n    if revision_type != \"neither\":\n        old_sentence = data.loc[i, \"original_sentence\"]\n        new_sentence = data.loc[i, \"revised_sentence\"]\n        composite_stmts = few_shot_prompt.format(old_sentence=old_sentence, new_sentence=new_sentence)\n        few_shot_prompts.append(composite_stmts)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Classify depending on the logprob difference","metadata":{}},{"cell_type":"code","source":"shortening_factor = 1\n\nfew_shot_preds = []\nfew_shot_logprob_diffs = []\n\nlogprob_threshold = 0\n\nfor i in range(len(few_shot_prompts)//shortening_factor):\n    few_shot_prompt = few_shot_prompts[i]\n\n    content_logprobs = torch.stack(get_completion_logprobs(prefix=few_shot_prompt, completion=\"substantive\")).to(torch_device)\n    surface_logprobs = torch.stack(get_completion_logprobs(prefix=few_shot_prompt, completion=\"superficial\")).to(torch_device)\n\n    logprob_diff = (torch.sum(content_logprobs) - torch.sum(surface_logprobs)).item()\n    few_shot_logprob_diffs.append(logprob_diff)\n\n    if logprob_diff > logprob_threshold:\n        few_shot_preds.append(\"content\")\n    else:\n        few_shot_preds.append(\"surface\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### ROC Curve for GPT-2, Few-shot Learning","metadata":{}},{"cell_type":"code","source":"plot_roc(actual_rev_types[:len(prompts)//shortening_factor], few_shot_logprob_diffs, metric_label=\"Logprob Diff (GPT-2 Few-Shot)\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Confusion Matrix for GPT-2, Few-shot Learning","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(actual_rev_types[:len(prompts)//shortening_factor], few_shot_preds, labels=[\"content\", \"surface\"])\ndisp = ConfusionMatrixDisplay(\n    confusion_matrix=cm, display_labels=[\"content\", \"surface\"]\n)\ndisp.plot();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analysis\nFew-shot prompting was even more effective than simply including reasoning in the prefix. The model was able to correctly classify the revision types with a higher accuracy when it was trained with in-context examples. Since it gives the chance for the model to learn the difference between the two revision types, the few-shot learning approach seems to be more effective in improving the model's performance than merely prepending descriptions.","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Completion Model 2: GEMMA-2b\nTo get an idea of how a different model might change performance on a prompt, we will use GEMMA-2b for comparison. GEMMA is a much newer model with many more parameters (albeit quantized, in our case), so we might expect it to perform even better as a “competent generalist” than GPT-2. This difference should help us get an idea of if or how performance on a prompt scales with more “advanced” models.","metadata":{}},{"cell_type":"markdown","source":"## Recall: GPT-2 Baseline Classification","metadata":{}},{"cell_type":"code","source":"plot_roc(actual_rev_types[:len(prompts)//shortening_factor], gpt2_logprob_diffs, metric_label=\"Logprob Diff\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cm = confusion_matrix(actual_rev_types[:len(prompts)//shortening_factor], gpt2_preds, labels=[\"content\", \"surface\"])\ndisp = ConfusionMatrixDisplay(\n    confusion_matrix=cm, display_labels=[\"content\", \"surface\"]\n)\ndisp.plot();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gemma Baseline Classification","metadata":{}},{"cell_type":"code","source":"%pip install -U bitsandbytes\n%pip install accelerate\n# %pip install -i https://pypi.org/simple/ bitsandbytes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Get Gemma 2B (without instruction tuning) Model","metadata":{}},{"cell_type":"code","source":"model_name = \"/kaggle/input/gemma/transformers/2b/2\"\n    \nquantization_config = BitsAndBytesConfig(load_in_8bit=True)\n    \ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=torch_device, quantization_config=quantization_config)\n\ntokenizer.decode([tokenizer.eos_token_id]);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Classify depending on the logprob difference","metadata":{}},{"cell_type":"code","source":"shortening_factor = 1\n\ngemma_preds = []\ngemma_logprob_diffs = []\n\nlogprob_threshold = 0\n\nfor i in range(len(prompts)//shortening_factor):\n    gemma_prompt = prompts[i]\n\n    content_logprobs = torch.stack(get_completion_logprobs(prefix=gemma_prompt, completion=\"is substantive\")).to(torch_device)\n    surface_logprobs = torch.stack(get_completion_logprobs(prefix=gemma_prompt, completion=\"is superficial\")).to(torch_device)\n\n    logprob_diff = (torch.sum(content_logprobs) - torch.sum(surface_logprobs)).item()\n    gemma_logprob_diffs.append(logprob_diff)\n\n    if logprob_diff > logprob_threshold:\n        gemma_preds.append(\"content\")\n    else:\n        gemma_preds.append(\"surface\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Load the classification data from json file since Gemma takes much time","metadata":{}},{"cell_type":"code","source":"with open(\"/kaggle/input/gemma-data/gemma_data.json\", \"r\") as file:\n    gemma_data = json.load(file)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### ROC Curve for Gemma Baseline","metadata":{}},{"cell_type":"code","source":"plot_roc(actual_rev_types[:len(prompts)], gemma_data[\"logprob_diffs\"], metric_label=\"Logprob Diff (GEMMA-2B Baseline)\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Confusion Matrix for Gemma Baseline","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(actual_rev_types[:len(prompts)], gemma_data[\"predictions\"], labels=[\"content\", \"surface\"])\ndisp = ConfusionMatrixDisplay(\n    confusion_matrix=cm, display_labels=[\"content\", \"surface\"]\n)\ndisp.plot();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# SBERT: Similarity Score Approach","metadata":{}},{"cell_type":"markdown","source":"## mpnet-base-v2\nWe will use mpnet-base-v2 as our model for the similarity score approach. mpnet-base-v2 is an SBERT model derived from the BERT architecture. BERT models compute sentence-level embeddings by pooling token-level embeddings, allowing them to capture the semantic meaning of sentences beyond that of individual tokens. The “S” in SBERT, which stands for “Siamese,” refers to the concept of “Siamese,” or “conjoined,” twins, which in turn alludes to the way in which SBERT models are trained. SBERT models are trained on “conjoined” pairs of sentences, using a loss function that quantifies a model’s “surprisal” on the type of relationship between those sentences. Thus, SBERT models use the sentence-level embeddings of the BERT architecture and a “conjoined” training approach to learn to output high similarity scores for semantically similar sentences ([Briggs](https://www.pinecone.io/learn/series/nlp/sentence-embeddings/)). mpnet-base-v2 is a strong general-purpose model described as “provid[ing] the best quality” on the [SBERT website](https://www.sbert.net/docs/pretrained_models.html), so we use it here.\n\nOur classification algorithm roughly adopts the following approach:\n* Compute the similarity score of the original sentence and the revision.\n* Invert the similarity score with a negative sign to measure “difference.”\n* If this “difference” is greater than a certain threshold, predict “content”; otherwise, predict “surface.”","metadata":{}},{"cell_type":"code","source":"old_sentences = []\nnew_sentences = []\nactual_rev_types = []\n\nfor i in range(len(data)):\n    revision_type = data.loc[i, \"revision_type\"]\n    if revision_type != \"neither\":\n        old_sentence = str(data.loc[i, \"original_sentence\"])\n        old_sentences.append(old_sentence)\n        new_sentence = str(data.loc[i, \"revised_sentence\"])\n        new_sentences.append(new_sentence)\n        actual_rev_types.append(revision_type)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Load Sentence Transformer Model","metadata":{}},{"cell_type":"code","source":"model = SentenceTransformer(\"all-mpnet-base-v2\").to(torch_device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Classification depending on the cosine similarity","metadata":{}},{"cell_type":"code","source":"shortening_factor = 1\nsbert_preds = []\ndiff_scores = []\n\ndiff_threshold = -0.661\n\nfor i in range(len(old_sentences)//shortening_factor):\n    \n    # Cosine-similarity code adapted from: https://www.sbert.net/docs/usage/semantic_textual_similarity.html\n    with torch.no_grad():\n        # Compute embeddings\n        original_embed = model.encode(old_sentences[i], convert_to_tensor=True, show_progress_bar=False).to(torch_device)\n        revision_embed = model.encode(new_sentences[i], convert_to_tensor=True, show_progress_bar=False).to(torch_device)\n\n        # Compute cosine-similarities\n        cos_similarity = util.cos_sim(original_embed, revision_embed)\n        diff_score = -cos_similarity[0].item()\n        diff_scores.append(diff_score)\n\n        if diff_score > diff_threshold:\n            sbert_preds.append(\"content\")\n        else:\n            sbert_preds.append(\"surface\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### ROC Curve for Sentence Transformer - Cosine Similarity","metadata":{}},{"cell_type":"code","source":"plot_roc(actual_rev_types[:len(old_sentences)//shortening_factor], diff_scores, metric_label=\"Semantic Diff (Sentence Transformer)\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Confusion Matrix for Sentence Transformer - Cosine Similarity","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(actual_rev_types[:len(old_sentences)//shortening_factor], sbert_preds, labels=[\"content\", \"surface\"])\ndisp = ConfusionMatrixDisplay(\n    confusion_matrix=cm, display_labels=[\"content\", \"surface\"]\n)\ndisp.plot();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analysis\n{TBA}","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Conclusion\nProviding descriptions of revision types and utilizing few-shot prompting improved the model’s classification capabilities. Adding descriptions (especially prepending) and in-context learning approach give the model the chance to learn the difference between the two revision cases.\n\nWe cannot assume identical prompting works equally well across language models. Opposed to our expectation, Gemma 2 did not perform as well as GPT-2 in classifying revision types, even though it was a bigger model. We will need to run more experiments to see if the prompt can be generalized across different models.\n\nClassification using cosine similarity is more reliable and interpretable than using the difference of log probs. While the difference of log probs is a common approach, it takes quite complicated steps to calculate and interpret the results. Cosine similarity is more straightforward and easier to understand, as well as performing significantly better in our experiments.","metadata":{}},{"cell_type":"markdown","source":"# Limitations\n### Dataset Limitations\nThe ArgRewrite corpus only contains essays written for a single argumentative prompt concerning self-driving cars. Thus, the results discussed in this notebook may not fully represent the models’ general classification abilities for essays of other subjects.\n\n### Lack of True Chain-of-Thought Prompting And Computational Limitations\nSuccessful chain-of-thought prompting requires language models to coherently generate intermediate reasoning of some sort to allow them to explicitly condition their final answer on that intermediate generation. More advanced instruction-tuned models are generally required to generate this kind of reasoning with any reliability. To keep this report self-contained, these models were not employed, as their usage would require access to processing power outside that which is freely available on Kaggle. Instead, we prompted GPT-2 with “prepackaged reasoning,” which provided definitional explanations for the completion terms “substantive” and “superficial.” This approach is far less adaptive than chain-of-thought prompting, as it only adds a small amount of boilerplate reasoning to the prompt without adapting to the particulars of the sentences under evaluation. Hence, this report does not provide any workflows to test more advanced models’ chain-of-thought capabilities.\n\n### Using Automation Responsibly\nThis report provides a possible workflow for automatically evaluating students’ revisions, which is not always desirable in every situation. The binary “content”/”surface” categorization lacks nuance and does not provide students with the individual feedback they might need to improve their revision, regardless of writing level. While the quantitative measures used to make the binary classification provide a slightly more nuanced continuous measure, they nonetheless reduce the complex process of revision to a single number that offers little direction for improvement. More complex, personalized feedback is required for students to grow as writers and revisers. Hence, our evaluation methodology is not intended to be used in educational contexts. It is intended to grant insights into broad patterns of revisions, rather than how to personally improve one’s own writing.","metadata":{}},{"cell_type":"markdown","source":"# Future Improvements\n### Essay-Level Evaluation\nThis report only analyzes sentence-level revision classification, but could potentially be expanded to generate and analyze essay-level revision scores.\n\n### Prompting Modern LLMs\nModern LLMs are much more “competent generalists” on NLP tasks than any model running in a self-contained Kaggle notebook, so further studies into the effectiveness of the “completion approach” might use the APIs of LLMs like GPT-4 or LLAMA-3 to get a better idea of how the very best models could perform on the classification task. These models more reliably generate intermediate reasoning as well, which would allow for additional experimentation with “chain-of-thought” prompting—possibly by prompting models to identify parts of a revision that would lead them to conclude a revision is “surface-level” or “content-level.”\n\n### Evaluating Performance on Revision Subcategories\nThe dataset of revisions used in this report included finer-grained revision categories not considered in our analysis. Our evaluation could benefit from additional, finer-grained analysis of the revision subclasses our models fail at most. With this information in mind, the models could be fine-tuned in an attempt to improve the their performance on those specific subclasses. This report does not use fine-tuning due to time constraints; however, the evaluations provided in this report provide a possible workflow which can be expanded to measure a fine-tuned model’s performance on revision subclasses. Thus, the process of fine-tuning and evaluating performance on revision subclasses will be left to future work.","metadata":{}},{"cell_type":"markdown","source":"# Appendix","metadata":{}},{"cell_type":"markdown","source":"## Data Wrangling","metadata":{}},{"cell_type":"code","source":"import requests\nimport zipfile\nimport io\nfrom pathlib import Path\nimport re\nimport warnings","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Download Dataset\n\nThe [ArgRewrite dataset](https://argrewrite.cs.pitt.edu/#corpus) was shared by the original authors under GNU GPLv3. Therefore, we have also released our wrangled version under the same license, which is accessible on Kaggle [here](https://www.kaggle.com/datasets/nghtctrl/argrewrite-v-2-corpus).","metadata":{}},{"cell_type":"code","source":"corpus_url = \"https://argrewrite.cs.pitt.edu/corpus/ArgRewrite.V2.zip\"\ncorpus_path = \"argrewrite-v-2-corpus\"\nresponse = requests.get(corpus_url)\n\nwith zipfile.ZipFile(io.BytesIO(response.content)) as z:\n    z.extractall(corpus_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Open annotated essay files","metadata":{}},{"cell_type":"code","source":"annotations_path = Path(corpus_path) / \"annotations\"\nxlsx_files = list(annotations_path.glob(\"**/12/*.xlsx\"))\n\nprint(f\"There are {len(xlsx_files)} xlsx files in the corpus.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Extract relevant data","metadata":{}},{"cell_type":"code","source":"%pip install -U openpyxl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_ids(prefix, filename):\n    \"\"\"Function used to retrieve writer IDs\"\"\"\n    regex = prefix + \"(\\d+)\"\n    return re.search(regex, filename).group(1)\n\n\nold_draft_dfs = []\nnew_draft_dfs = []\n\nfor xlsx_file in xlsx_files:\n    try:\n        # Ignore all of the openpyxl incompatibility warnings.\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\",\n                category=UserWarning,\n                module=re.escape(\"openpyxl.styles.stylesheet\"),\n            )\n            with open(xlsx_file, \"rb\") as f:\n                # Open each of the \"Old Draft\" and \"New Draft\" sheets\n                old_draft_sheet = pd.read_excel(f, sheet_name=\"Old Draft\")\n                new_draft_sheet = pd.read_excel(f, sheet_name=\"New Draft\")\n                \n                # Extract the writer IDs from the file path\n                writer_id = get_ids(\"Annotation_2018argrewrite_\", str(f))\n                \n                old_draft_df = pd.DataFrame(\n                    {\n                        \"writer_id\": writer_id,\n                        \"original_sentence_index\": old_draft_sheet[\"Sentence Index\"].astype(str),\n                        \"revised_sentence_index\": old_draft_sheet[\"Aligned Index\"].astype(str),\n                        \"original_sentence\": old_draft_sheet[\"Sentence Content\"],\n                        \"revision_purpose\": old_draft_sheet[\"Revision Purpose Level 0\"],\n                        \"revision_operation\": old_draft_sheet[\"Revision Operation Level 0\"],\n                    }\n                )\n                old_draft_dfs.append(old_draft_df)\n\n                new_draft_df = pd.DataFrame(\n                    {\n                        \"writer_id\": writer_id,\n                        \"original_sentence_index\": new_draft_sheet[\"Aligned Index\"].astype(str),\n                        \"revised_sentence_index\": new_draft_sheet[\"Sentence Index\"].astype(str),\n                        \"revised_sentence\": new_draft_sheet[\"Sentence Content\"],\n                        \"revision_purpose\": new_draft_sheet[\"Revision Purpose Level 0\"],\n                        \"revision_operation\": new_draft_sheet[\"Revision Operation Level 0\"],\n                    }\n                )\n                new_draft_dfs.append(new_draft_df)\n    except ValueError as e:\n        # Catch invalid files\n        print(f\"Error in {xlsx_file}: {e}\")\n\nold_draft_df = pd.concat(old_draft_dfs, ignore_index=True)\nnew_draft_df = pd.concat(new_draft_dfs, ignore_index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"old_draft_df has {old_draft_df.shape[0]} rows.\")\nprint(f\"new_draft_df has {new_draft_df.shape[0]} rows.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Retrieve and combine revised sentences","metadata":{}},{"cell_type":"code","source":"# Find rows where revision_sentence was indicated to be revised from 2+ original_sentences, indicated by ',' separator\nmultiple_original_indices = new_draft_df[\"original_sentence_index\"].str.contains(\n    \",\", na=False\n)\n\n# Create dataframe storing rows of new_draft_df with sentences revised from 2+ multiple original_sentences\ncombined_sentences_df = new_draft_df.loc[multiple_original_indices]\n\n# Each item in the list will be a string of sentences that were combined in the new_draft_df\ncombined_sentence_strings = []\n\n# For each case where sentences were combined:\nfor row in combined_sentences_df.iloc:\n    writer_id = row[\"writer_id\"]  # Get id of writer who combined sentences\n    original_index_group = row[\"original_sentence_index\"].split(\n        \",\"\n    )  # Get indices of writer's combined sentences\n    source_sentences = \"\"  # Temp string for sentences that were combined\n\n    # For each of the indices of the writer's combined sentences:\n    for original_index in original_index_group:\n        # Determine row of next combined sentence via id of combining writer and index of next combined sentence\n        to_combine = (old_draft_df[\"writer_id\"] == writer_id) & (\n            old_draft_df[\"original_sentence_index\"] == original_index\n        )\n        # Add the row's sentence to string storing sentences that were combined\n        source_sentences += (\n            \" \" + old_draft_df.loc[to_combine][\"original_sentence\"].values[0]\n        )\n\n    combined_sentence_strings.append(source_sentences)\n\n# For rows with multiple values in original_sentence_index, force original_sentence_index to only the first index listed\nnew_draft_df.loc[multiple_original_indices, \"original_sentence_index\"] = (\n    new_draft_df.loc[multiple_original_indices, \"original_sentence_index\"]\n    .str.split(\",\")\n    .str[0]\n)\n\nprint(f\"old_draft_df has {old_draft_df.shape[0]} rows.\")\nprint(f\"new_draft_df has {new_draft_df.shape[0]} rows.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Join the original and revised sentences","metadata":{}},{"cell_type":"code","source":"# Do a full outer join based on the sentence indexes\nsentence_pair_df = new_draft_df.merge(\n    old_draft_df[[\"writer_id\", \"original_sentence\", \"original_sentence_index\"]],\n    how=\"outer\",\n    left_on=[\"writer_id\", \"original_sentence_index\"],\n    right_on=[\"writer_id\", \"original_sentence_index\"],\n)\n\nprint(f\"sentence_pair_df has {sentence_pair_df.shape[0]} rows.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Do the following for all cases of combined_sentences:\nfor i in range(len(combined_sentences_df[\"writer_id\"])):\n    writer_id = combined_sentences_df[\"writer_id\"].iloc[\n        i\n    ]  # Get id of writer who combined sentences\n    revised_sentence_id = combined_sentences_df[\"revised_sentence_index\"].iloc[\n        i\n    ]  # Get id of revised combined sentence\n\n    # Get row of sentence revised by combining old sentences\n    revised_row = (sentence_pair_df[\"writer_id\"] == writer_id) & (\n        sentence_pair_df[\"revised_sentence_index\"] == revised_sentence_id\n    )\n    # Retrieve string of the old sentences that were combined\n    revised_sentence = combined_sentence_strings[i]\n\n    # Set original_sentence in revised sentence's row to be the sentences the revised sentence combined\n    sentence_pair_df.loc[revised_row, \"original_sentence\"] = revised_sentence\n\n    # Debug message printing each modified row. The revised sentence should share content with the source sentences.\n    print(\n        \"Source sentences:\",\n        sentence_pair_df.loc[revised_row][\"original_sentence\"].values[0],\n        \"\\nRevised sentence:\",\n        sentence_pair_df.loc[revised_row][\"revised_sentence\"].values[0],\n    )\n    print()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the number of each revision subtypes\nsentence_pair_df[\"revision_purpose\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def classify_purpose(x):\n    \"\"\"This function converts revision subtypes as either 'content', 'surface', or 'neither'\"\"\"\n    if isinstance(x, str):\n        # Simple hack to match the subwords; however, there might be a better way to do this\n        if any(\n            word in x\n            for word in [\"Clai\", \"Warr\", \"Evid\", \"Rebu\", \"Prec\", \"Cont\", \"Orga\"]\n        ):\n            return \"content\"\n        else:\n            return \"surface\"\n    else:\n        return \"neither\"\n\n\n# Insert the newly converted revision types into the sentence pairs dataset\nrevision_type = sentence_pair_df[\"revision_purpose\"].apply(classify_purpose)\nassert len(revision_type) == sentence_pair_df.shape[0]\nsentence_pair_df[\"revision_type\"] = revision_type\n\nsentence_pair_df[\"revision_type\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Save as csv file","metadata":{}},{"cell_type":"code","source":"sentence_pair_df.to_csv(\"sentence_pairs.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}